---
title: |
       |
       |
       |
       | MATH1318 - Time Series Analysis 
       |
       |
       |
       | Assignment 2
       
author: |
   | 
   | 
   | 
   | 
   | 
   | 
   | 
   | 
   | 
   | 
   | 
   | 
   | Kavinda Goonesekere
   | S3987368
date: "2024-05-15"
output: html_document
---

<style type="text/css">

h1.title {
  font-size: 58px;
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 34px;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 34px;
}
body{
  font-family: Helvetica;
  font-size: 14pt;
}
</style>

<!-- --- -->
<!-- title: "S3987368 Assignment 1" -->
<!-- author: "Kavinda Goonesekere" -->
<!-- date: "2024-03-23" -->
<!-- output: html_document -->
<!-- --- -->

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(TSA)
library(tseries)
library('rlist')
library(forecast)
library(lmtest)
library(data.table)
library(Metrics)
```

## Introduction

### Overview

This report is concerned with performing exploratory analysis and exploring feasible ARIMA models for a dataset describing yearly Global Land Temperature Anomalies in Degrees Celsius against the base period 1901-2000. The data provided contains temperature anomalies covering the years from 1850 to 2023, with each anomaly representing the deviation of temperature in Degrees Celsius for that year compared to the mean temperature of the baseline period (1901-2000). 


### Objectives

* Perform descriptive analysis of the dataset.
* Shortlist ARMA and ARIMA models and analyse the models for selection using ACF-PACF, EACF, BIC etc.
* Fit the selected models and find parameter estimates
* Utilize goodness-of-fit metrics such as AIC, BIC, MSE etc. to select the optimal model.

\newpage

## Method and Results

### Visualising the Series

In the first step, the dataset is loaded into the environment and summary obtained. The summaries show a mean anomaly value of 0.06218°C, a max of 0.91°C and a min of -0.44°C. The 1st quartile for anomalies is -0.12750°C and the 3rd quartile is 0.23°C. The median anomaly value is 0.

```{r, echo=FALSE}
setwd("C:/Work/MC242/Sem3/Time Series Analysis/TS Assignment 2")
```
```{r}
anomalies = read.csv("assignment2Data2024.csv", col.names = c("Year", "Anomaly"))
summary(anomalies)

```
The anomaly column is converted to a time-series object with a frequency of 1 since it is annual data. The series is then plotted to observe any trends or seasonality in Figure 1. The initial plot demonstrates a slowly decreasing trend till the early 1900s, after which a linear increasing trend is seen. The series seems to have both AR and MA components since there are also fluctuations in the series throughout. Not much changing variance is observed.

```{r, fig.cap = "Figure 1: Plot of the annual anomalies in degrees Celsius"}
anomalies.ts = ts(anomalies[,c(2)], frequency = 1, start = 1850, end = 2023)

plot(anomalies.ts, type='o', ylab='Anomalies in degrees Celsius', xlab='Year', main = "Time series plot of annual anomalies in degrees Celsius")
```

### Verifying Stationarity and Normality

The ACF and PACF plots for the series are shown in Figure 2. The ACF plot returns a slowly decaying plot with most peaks beyond the confidence intervals and the PACF plot produces two significant peaks beyond the confidence intervals at the first and third lags. Both of these suggest non-stationarity in the series due to the autocorrelations observed at multiple lags.

```{r, fig.cap = "Figure 2: ACF plot of the time series anomalies data"}
par(mfrow=c(2,1), mar=c(3,3,3,1))
acf(anomalies.ts, main="ACF plot", lag.max = 40)
pacf(anomalies.ts, main="PACF plot", lag.max = 40)
par(mfrow=c(1,1))
```
Checking the stationarity using an Automated Dickey-Fuller (ADF) test reveals a p-value of 0.9044, affirming the null hypothesis at the 5% significance level. This confirms that the series is not stationary. 

The QQ plot of the series in Figure 3 shows both tails deviating from the reference line suggesting visually that the data points are not normally distributed. A Shapiro-Wilk test confirms this producing a p-value <0.05, affirming the alternate hypothesis that the series is not normally distributed.  

```{r, fig.cap = "Figure 3: Q-Q plot of the time series anomalies data"}
adf.test(anomalies.ts)

qqnorm(anomalies.ts, ylab="Anomalies in degrees Celsius", xlab="Normal Scores")
qqline(anomalies.ts)

shapiro.test(anomalies.ts) 
```
### Transforming the Series

A Box-Cox transformation may help with achieving normality in the data. A Box-Cox transformation is applied to a shifted version of the series (to ensure all values are positive), producing a lambda value of 1. A lambda value of 1 indicates no transformation, so the Box-Cox transformation is not carried out on the series. 

A log transformation is then applied. The time series plot for the log transformed series is shown in Figure 4, its Q-Q plot vs the original Q-Q plot is shown in Figure 5, and ACF and PACF plots for the log transformed series are shown in Figure 6. The plot of the log transformed series does not look more stabilized compared to the original plot but has a significant dip in the early 1900s. The QQ plots compared side by side show that the Log transformed series deviates less from the reference line compared to the non-transformed series, indicating that it is more normally distributed. However overall, the transformed series is still not normally distributed. A Shapiro-Wilk test performed on the transformed series still shows a very small p-value (1.052e-15), confirming non-normal distribution. Performing an ADF test reveals that the series is still not stationary due to a high p-value (0.2184). 

```{r, fig.width=10, fig.height=4, fig.cap = c("Figure 6: ACF and PACF plots for log transformed anomalies series", "Figure 4: Time series plot of log transformed anomalies series", "Figure 5: Q-Q plots for log transformed anomalies series vs original series")}
summarise_series = function(series, transform) { 
  plot(series, type='o',ylab = "Anomalies in degrees Celsius", main=paste('Time series plot of ', transform, ' of anomalies series')) 
  par(mfrow=c(1,2), mar=c(3,4,3,1))
  qqnorm(series, main=paste('Q-Q plot of ', transform, ' series'), ylab=paste(transform, ' anomalies'), xlab="Normal Scores")
  qqline(series)
  qqnorm(anomalies.ts, main="Original series Q-Q plot", ylab="Anomalies in degrees Celsius", xlab="Normal Scores")
  qqline(anomalies.ts)
  par(mfrow=c(1,1))

  par(mfrow=c(1,2), mar=c(3,3,5,1))
  acf(series, main=paste('ACF of the ', transform, ' series.'))
  pacf(series, main=paste('PACF of the ', transform, ' series.'))
  par(mfrow=c(1,1))
  
  print(shapiro.test(series)) 
  print(adf.test(series))
}

adjusted.series = anomalies.ts+abs(min(anomalies.ts))+0.001
BC = BoxCox.ar(adjusted.series) 
BC$ci
lambda <- BC$lambda[which(max(BC$loglike) == BC$loglike)]
lambda

log.anomalies = log(adjusted.series)
summarise_series(log.anomalies, "Log")
```
### Differencing the Series

The first difference of the series is obtained to attempt at making the series stationary. The time series plot for the 1st-differenced series is shown in Figure 7, its Q-Q plot vs the original Q-Q plot is shown in Figure 8, and ACF and PACF plots for the 1st-differenced series are shown in Figure 9. The 1st difference of the anomalies series is plotted and is visually observed to have a constant mean and not much changing variance. This is confirmed with the results of the ADF test producing a p-value of 0.01, indicating that the null hypothesis is rejected and the 1st-differenced series is stationary. The Q-Q plot of the 1st-differenced series deviates less from the reference line compared to the original series and can be said to be more normal. The results of the Shapiro-Wilk test confirm normality, with a high p-value of 0.3249. The ACF plot for the 1st-differenced series shows a significant peak at the second lag and a borderline peak at the fourth lag. A couple of significant peaks are also observed at higher lags but these are ignored as they are late lags. The PACF plot is very similar to the ACF plot, with a significant peak at the second lag. Therefore, q can be assumed to be 2 and p can be taken to be 1 based on the ACF and PACF plots respectively. These p and q values suggest the modeling of ARIMA(1,1,1) and ARIMA(1,1,2)

Since the first difference of the anomalies series provided satisfactory results in terms of normality and stationarity, no more differencing is performed in order to avoid over-differencing. 

```{r, fig.width=10, fig.height=4, fig.cap = c("Figure 7: Time series plot of 1st-differenced anomalies series", "Figure 8: Q-Q plots for 1st-differenced anomalies series vs original series", "Figure 9: ACF and PACF plots for 1st-differenced anomalies series")}
diff.anomalies = diff(anomalies.ts, differences = 1)
summarise_series(diff.anomalies, "1st difference")
```

\newpage

### EACF Table

The EACF table for the differenced series is shown below. The top-left 'o' is obtained from the EACF table and this is observed to occur at AR = 0 and MA = 2. This results in the possible models being ARIMA(0,1,2), ARIMA(1,1,2), ARIMA(1,1,3), and ARIMA(0,1,3) based on the best model and its neighbors.

```{r}
eacf(diff.anomalies)
```
The BIC table for the differenced series is shown in Figure 10. Plotting the BIC table for the differenced series shows that the top 4 models all have p=2. The second best model also has error lag of 5. Based on this, the shortlisted models are ARIMA(2,1,0), ARIMA(2,1,4).

```{r, fig.cap = "Figure 10: BIC table for 1st-differenced anomalies series"}
res = armasubsets(y=diff.anomalies, nar=5, nma=5, y.name='p', ar.method='ols')
plot(res)
```

\newpage

### Parameter Estimation

Based on the above model selection, the final models for modeling are listed below:

* ARIMA(0,1,2)
* ARIMA(1,1,1)
* ARIMA(1,1,2)
* ARIMA(0,1,3)
* ARIMA(2,1,0)
* ARIMA(2,1,4)

**ARIMA(0,1,2)**

Initially, the ARIMA(0,1,2) model is fitted and the coefficients are found using the coeftest() function using the ML estimator. This produces coefficients for the MA1 and MA2 components that are both significant, with MA1 being less significant than MA2. Using the CSS estimator produces roughly similar p-values for each coefficient.

```{r}
model.012ML = Arima(anomalies.ts, order=c(0,1,2), method='ML')
model.012ML
coeftest(model.012ML)

model.012CSS = Arima(anomalies.ts, order=c(0,1,2), method='CSS')
model.012CSS
coeftest(model.012CSS)

```
**ARIMA(1,1,1)**

The ARIMA(1,1,1) model is fitted and the coefficients are found using the coeftest() function using the ML estimator. This produces coefficients for the AR1 and MA1 components that are both significant, with AR1 being less significant than MA2, but both are highly significant regardless. Using the CSS estimator produces roughly similar p-values for each coefficient.

```{r}
model.111ML = Arima(anomalies.ts, order=c(1,1,1), method='ML')
model.111ML
coeftest(model.111ML)

model.111CSS = Arima(anomalies.ts, order=c(1,1,1), method='CSS')
model.111CSS
coeftest(model.111CSS)

```
**ARIMA(2,1,1) & ARIMA(2,1,2) (Neighbor Models)**

Since both AR1 and MA1 coefficients of the ARIMA(1,1,1) model are highly significant, neighboring models ARIMA(2,1,1) and ARIMA(2,1,2) are added to the list of possible models to be fitted. ARIMA(2,1,1) produces a significant AR2 coefficient with a very low p-value but the AR1 and MA1 coefficients are insignificant. The ARIMA(2,1,2) model produces coefficients for AR1, AR2, MA1, and MA2 which are all insignificant, with AR2 being borderline with a p-value of 0.07585 for the ML estimator and 0.0558 for the CSS estimator. Due to the borderline p-value for AR2, the CSS-ML estimator is also used to see if it is possible to obtain a significant coefficient for AR2. However, it produces a p-value of 0.07516, mirroring the ML estimator. The CSS and ML estimators produce roughly the same p-values for all five models tested here. Neither of the neighboring models tested seem to improve upon the ARIMA(1,1,1) model due to the presence of insignificant coefficients.
```{r}
model.211ML = Arima(anomalies.ts, order=c(2,1,1), method='ML')
model.211ML
coeftest(model.211ML)

model.211CSS = Arima(anomalies.ts, order=c(2,1,1), method='CSS')
model.211CSS
coeftest(model.211CSS)

model.212ML = Arima(anomalies.ts, order=c(2,1,2), method='ML')
model.212ML
coeftest(model.212ML)

model.212CSS = Arima(anomalies.ts, order=c(2,1,2), method='CSS')
model.212CSS
coeftest(model.212CSS)

model.212CSSML = Arima(anomalies.ts, order=c(2,1,2), method='CSS-ML')
model.212CSSML
coeftest(model.212CSSML)
```
**ARIMA(1,1,2)**

The ARIMA(1,1,2) model is fitted and the coefficients are found using the coeftest() function using the ML estimator. This produces a highly significant coefficient for only the MA2 component, while AR1 and MA1 components are both insignificant. Using the CSS estimator produces roughly similar p-values for each coefficient.

```{r}
model.112ML = Arima(anomalies.ts, order=c(1,1,2), method='ML')
model.112ML
coeftest(model.112ML)

model.112CSS = Arima(anomalies.ts, order=c(1,1,2), method='CSS')
model.112CSS
coeftest(model.112CSS)
```
**ARIMA(0,1,3)**

The ARIMA(0,1,3) model is fitted and the coefficients are found using the coeftest() function using the ML estimator. This produces a highly significant coefficient for only the MA2 component, while MA1 and MA3 components are both insignificant. Using the CSS estimator produces roughly similar p-values for each coefficient.

```{r}
model.013ML = Arima(anomalies.ts, order=c(0,1,3), method='ML')
model.013ML
coeftest(model.013ML)

model.013CSS = Arima(anomalies.ts, order=c(0,1,3), method='CSS')
model.013CSS
coeftest(model.013CSS)
```
**ARIMA(2,1,0)**

The ARIMA(2,1,0) model is fitted and the coefficients are found using the coeftest() function using the ML estimator. This produces a highly significant coefficient for only the AR2 component, while AR1 component is insignificant. Using the CSS estimator produces roughly similar p-values for each coefficient.

```{r}
model.210ML = Arima(anomalies.ts, order=c(2,1,0), method='ML')
model.210ML
coeftest(model.210ML)

model.210CSS = Arima(anomalies.ts, order=c(2,1,0), method='CSS')
model.210CSS
coeftest(model.210CSS)
```
**ARIMA(2,1,4)**

The ARIMA(2,1,4) model is fitted and the coefficients are found using the coeftest() function using the ML estimator. This produces no significant coefficients for any of the components. Using the CSS estimator produces different, but highly insignificant p-values for each coefficient. Due to the fact that the p-values are high, CSS-ML is also explored as an alternative estimator since there is a difference between CSS and ML results and it reflects the significance values obtained from CSS.

```{r}
model.214ML = Arima(anomalies.ts, order=c(2,1,4), method='ML')
model.214ML
coeftest(model.214ML)

model.214CSS = Arima(anomalies.ts, order=c(2,1,4), method='CSS')
model.214CSS
coeftest(model.214CSS)

model.214CSSML = Arima(anomalies.ts, order=c(2,1,4), method='CSS-ML')
model.214CSSML
coeftest(model.214CSSML)

```
### AIC and BIC Analysis

The AIC() and BIC() functions are used to determine the AIC and BIC values for each model. The sort.score() function developed by Yong Kai Wong is utilized to sort the AIC and BIC scores in ascending order in order to aid selection of the model with the least AIC and BIC (Canvas, 2024). Model ARIMA(2,1,0) has the lowest scores for both AIC and BIC.

```{r}

sort.score <- function(x, score = c("bic", "aic")){
  if (score == "aic"){
    x[with(x, order(AIC)),]
  } else if (score == "bic") {
    x[with(x, order(BIC)),]
  } else {
    warning('score = "x" only accepts valid arguments ("aic","bic")')
  }
}

sort.score(AIC(model.012ML, model.111ML, model.211ML, model.212ML, 
               model.112ML, model.013ML, model.210ML, model.214ML), 
           score = "aic")

sort.score(BIC(model.012ML, model.111ML, model.211ML, model.212ML, 
               model.112ML, model.013ML, model.210ML, model.214ML), 
           score = "bic")
```

### Error Metric Comparison

The accuracy() function is given each fitted model to obtain the Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1). 
```{r}
Smodel.012A = forecast::accuracy(model.012ML)[1:7]
Smodel.111A = forecast::accuracy(model.111ML)[1:7]
Smodel.211A = forecast::accuracy(model.211ML)[1:7]
Smodel.212A = forecast::accuracy(model.212ML)[1:7]
Smodel.112A = forecast::accuracy(model.112ML)[1:7]
Smodel.013A = forecast::accuracy(model.013ML)[1:7]
Smodel.210A = forecast::accuracy(model.210ML)[1:7]
Smodel.214A = forecast::accuracy(model.214ML)[1:7]

df.Smodels = data.frame(
                rbind(Smodel.012A, Smodel.111A, Smodel.211A, Smodel.212A, 
                      Smodel.112A, Smodel.013A, Smodel.210A, Smodel.214A)
                )
colnames(df.Smodels) = c("ME", "RMSE", "MAE", "MPE", "MAPE", "MASE", "ACF1")
rownames(df.Smodels) = c("ARIMA(0,1,2)", "ARIMA(1,1,1)", "ARIMA(2,1,1)", "ARIMA(2,1,2)", 
                         "ARIMA(1,1,2)", "ARIMA(0,1,3)", "ARIMA(2,1,0)", "ARIMA(2,1,4)")

df.Smodels
```

The resulting values show that MPE and MAPE have either infinity or NaN instead of numeric values. This is due to the nature of the MPE equation which which has the actual data point value of the time series ${y}_i$ in the denominator (MPE - Mean Percentage Error — Permetrics 2.0.0 Documentation, n.d.). The MAPE equation suffers from the same problem (MAPE - Mean Absolute Percentage Error — Permetrics 2.0.0 Documentation, n.d.). Both equations are shown below. Since the anomalies time series contains several instances of the value 0, this leads to an undefined result for both MPE and MASE. 

\begin{equation}
\text{MPE}(y, \hat{y}) = \frac{100\%}{N} \sum_{i=0}^{N - 1} \frac{y_i - \hat{y}_i}{y_i}  
\end{equation}

\newline

\begin{equation}
\text{MAPE}(y, \hat{y}) = \frac{100\%}{N} \sum_{i=0}^{N - 1} \frac{|y_i - \hat{y}_i|}{|y_i|}
\end{equation}

Therefore, MAPE is replaced by Symmetrical Mean Absolute Percentage Error (SMAPE), which calculates a similar metric but is instead weighted with the sum of the absolute values of the actual value ${y}_i$ and predicted value $\hat{y}_i$ as shown below (SMAPE - Symmetric Mean Absolute Percentage Error — Permetrics 2.0.0 Documentation, n.d.):

\begin{equation}
\text{SMAPE}(y, \hat{y}) = \frac{100\%}{N} \sum_{i=0}^{N - 1} \frac{ 2*|y_i - \hat{y}_i|}{|y| + |\hat{y}|}
\end{equation}

The SMAPE values are calculated below for each model.

```{r}
smapes = c(smape(anomalies.ts, fitted(model.012ML)),
           smape(anomalies.ts, fitted(model.111ML)),
           smape(anomalies.ts, fitted(model.211ML)),
           smape(anomalies.ts, fitted(model.212ML)),
           smape(anomalies.ts, fitted(model.112ML)),
           smape(anomalies.ts, fitted(model.013ML)),
           smape(anomalies.ts, fitted(model.210ML)),
           smape(anomalies.ts, fitted(model.214ML))
         )
df.Smodels = cbind(df.Smodels, data.frame(SMAPE = smapes)) 
setDT(df.Smodels, keep.rownames = "Models")[]
```
The error metrics are individually sorted in ascending order and just the top four models chosen to determine if there are any models which repeatedly show up in the top four. MPE is omitted due to its undefined values. ARIMA(2,1,0) is the top model for the ME, ACF1, and SMAPE error metrics. However for those metrics, ARIMA(2,1,4) still appears in the top 4 models. ARIMA(2,1,4) is the top model for RMSE, MAE, and MASE error metrics. However for those metrics, ARIMA(2,1,0) still appears in the top 4 models as well.  

Despite the good performance of ARIMA(2,1,4) on these error metrics, it appears second-to-last on the AIC ranking and last in the BIC ranking. Therefore, ARIMA(2,1,0) is considered to be the best model from all the models fitted after considering all measures (AIC, BIC, and error metrics).

```{r}
df.Smodels[order(df.Smodels$ME),][1:4,c(1,2)]
df.Smodels[order(df.Smodels$RMSE),][1:4,c(1,3)]
df.Smodels[order(df.Smodels$MAE),][1:4,c(1,4)]
df.Smodels[order(df.Smodels$MASE),][1:4,c(1,7)]
df.Smodels[order(df.Smodels$ACF1),][1:4,c(1,8)]
df.Smodels[order(df.Smodels$ACF1),][1:4,c(1,9)]
```

\newpage

## Conclusion

During the investigation, a total of eight models were fitted and tested for the time series data describing yearly Global Land Temperature Anomalies in Degrees Celsius against the base period 1901-2000. Visual analysis of the series confirmed a possible combination of AR and MA components due to the trend and fluctuations seen in the series. ADF testing confirmed non-stationarity in the series and Shapiro-Wilk testing confirmed non-normality in the data. This was further supported by the ACF and PACF plots which demonstrated autocorrelations at multiple lags. Applying the Box-Cox transformation to the series provided a lambda value of 1, indicating no transformation, and was therefore abandoned. Log transforming the series provided visual improvements but further testing confirmed that the log-transformed series was still not stationary and had non-normal distribution. The first difference of the anomalies series, however, produced a stationary series with normal distribution and was chosen as the suitable transformation for the series. The ACF and PACF plots of the differenced series, in addition to the EACF table and neighboring model selection, provided a list of eight total models for fitting. These models were fitted and their parameter coefficients were discussed. Afterwards, obtaining the AIC and BIC scores, along with the ME, RMSE, MAE, MASE, ACF1, and SMAPE error metrics shortlisted the two best models to be ARIMA(2,1,0) and ARIMA(2,1,4). However, the superior performance of ARIMA(2,1,0) when considering AIC and BIC scores and its overall satisfactory performance with the other error metrics justified its selection as the most suitable model to describe the yearly Global Land Temperature Anomalies data.

## References

* Canvas (2024) sort.score.R. Available at: https://rmit.instructure.com/courses/124176/files/36179115?module_item_id=5935464 (accessed 1st May 2024).
* MPE - Mean Percentage Error — Permetrics 2.0.0 documentation. (n.d.). Permetrics.readthedocs.io. Retrieved May 5, 2024, from https://permetrics.readthedocs.io/en/latest/pages/regression/MPE.html
* MAPE - Mean Absolute Percentage Error — Permetrics 2.0.0 documentation. (n.d.). Permetrics.readthedocs.io. Retrieved May 5, 2024, from https://permetrics.readthedocs.io/en/latest/pages/regression/MAPE.html
* SMAPE - Symmetric Mean Absolute Percentage Error — Permetrics 2.0.0 documentation. (n.d.). Permetrics.readthedocs.io. https://permetrics.readthedocs.io/en/latest/pages/regression/SMAPE.html

